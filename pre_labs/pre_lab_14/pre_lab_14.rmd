---
title: "pre_lab_14.Rmd"
author: "derek willis"
date: "11/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# An intro to text analysis

Throughout this course, we've been focused on finding information in structured data. We've learned a lot of techniques to do that, and we've learned how the creative mixing and matching of those skills can find new insights.

What happens when the insights are in unstructured data? Like a block of text?

Turning unstructured text into data to analyze is a whole course in and of itself -- and one worth taking if you've got the credit hours -- but some simple stuff is in the grasp of basic data analysis.

To do this, we'll need a new library -- [tidytext](https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html), which you can guess by the name plays very nicely with the tidyverse. So install it with `install.packages("tidytext")` and we'll get rolling.

### Task 1: Load libraries and settings
**Task** Run the following code in the gray-colored codeblock below to load the libraries we'll use.

```{r}
#install.packages("tidytext")
library(tidyverse)
library(tidytext)
library(janitor)
library(lubridate)
```

Here's the question we're going to go after: How did federal politicians talk about the coronavirus pandemic on Twitter?

To answer this question, we'll use a dataset of tweets posted by federal politicians from both campaign and official accounts that mentioned either "COVID" or "coronavirus" beginning on Feb. 1, 2020. This dataset doesn't include retweets, only original tweets. Let's read in this data and examine it:

### Task 2: Read in data
**Task** Run the following code and describe the dataframe it outputs.
**Answer** It is a dataframe with politician usernames and their tweets that have COVID or coronavirus in it. It also has the date and time of the tweet, the branch the politician is in, the state they represent, their name, their gender and their party. 

```{r}
covid_tweets <- read_rds('data/covid_tweets.rds')
```

What we want to do is to make the `content` column easier to analyze. Let's say we want to find out the most commonly used words. We'll want to remove URLs from the text of the tweets since they aren't actual words. Let's use mutate to make that happen:


### Task 3: Remove URLs from content
**Task** Run the following code.

```{r}
covid_tweets <- covid_tweets %>%
  mutate(content = gsub("http.*","", content))
```

If you are trying to create a list of unique words, R will treat differences in capitalization as unique and also will include punctuation by default, even using its `unique` function:

### Task 4: Trying out unique
**Task** Run the following code and describe what the `unique` function does to the original list of words.
**Answer** It creates a list of the words, but it leaves out repeats of words. So it only lists one of the words "dog" and "cat" which are both repeated twice. It does not consider case when it is decided what is a repeat.

```{r}
a_list_of_words <- c("Dog", "dog", "dog", "cat", "cat", ",")
unique(a_list_of_words)
```

Fortunately, this is a solved problem with tidytext, which has a function called `unnest_tokens` that will convert the text to lowercase and remove all punctuation. The way that `unnest_tokens` works is that we tell it what we want to call the field we're creating with this breaking apart, then we tell it what we're breaking apart -- what field has all the text in it. For us, that's the `content` column:

### Task 5: Trying out unnest_tokens
**Task** Run the following code and describe the output of using the `unnest_tokens` function.
**Answer** This function makes all of the words lowercase and removes punctuation from the words we want to list.

```{r}
unique_words <- covid_tweets %>% select(content) %>%
  unnest_tokens(word, content)
View(unique_words)
```

### Task 6: Make a column chart
**Task** Run the following code and describe what the resulting graphic shows. Is it interesting?
**Answer** No it isn't interesting. The top words aren't unusual or rare words that mean anything. They're just normal articles like the, to, and.

Now we can look at the top words in this dataset. Let's limit ourselves to making a plot of the top 25 words, and we'll use the function `count` to do the counting:

```{r}
unique_words %>%
  count(word, sort = TRUE) %>%
  top_n(25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(x = word, y = n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
      labs(x = "Count",
      y = "Unique words",
      title = "Count of unique words found in tweets")
```

Well, that's a bit underwhelming - a lot of very common (and short) words. This also is a solved problem in working with text data, and words like "a" and "the" are known as "stop words". In most cases you'll want to remove them from your analysis since they are so common. Tidytext provides a dataframe of them:

### Task 7: Load the stop words
**Task** Run the following code

```{r}
data("stop_words")
```

Then we're going to use a function we haven't used yet called an `anti_join`, which filters out any matches. So we'll `anti_join` the stop words and get a list of words that aren't stop words.

From there, we can get a simple word frequency by just grouping them together and counting them. We can borrow the percent code from above to get a percent of the words our top 10 words represent.

### Task 8: Using anti_join
**Task** Run the following code and describe the results. Is it more interesting than before?
**Answer** It is more interesting than before. It is showing more uncommon words that are relevant to what we're looking for. There are multiple rows for things that reprensent COVID-19, so this might make it a little more difficult to analyze like this.

```{r}
unique_words %>%
  anti_join(stop_words) %>%
  group_by(word) %>%
  tally(sort=TRUE) %>%
  mutate(percent = (n/sum(n))*100) %>%
  top_n(10)
```

Those seem like more relevant unique words. Now, here's where we can start to do more interesting and meaningful analysis. Let's create two dataframes of unique words based on time: one for all of 2020 and the other for all of 2021:

### Task 9: Create dataframes for 2020 and 2021
**Task** Run the following code

```{r}
unique_words_2020 <- covid_tweets %>%
  filter(created < '2021-01-01') %>%
  select(content) %>%
  unnest_tokens(word, content)

unique_words_2021 <- covid_tweets %>%
  filter(created >= '2021-01-01') %>%
  select(content) %>%
  unnest_tokens(word, content)
```

Then we can create top 10 lists for both of them and compare:

### Task 10: Create dataframes with the top 10 words in each year
**Task** Run the following code and describe the results.
**Answer** The results are sort of similar, with both lists having multiple entries of words that represent COVID-19, like COVID, coronavirus, pandemic, health. I think everything in the lists were the same except the 2020 list had crisis, which the 2021 list did not have.Additionally, the 2021 list had vaccine or some variation, which the 2020 list did not have. Additionally, vaccine was more notable in the 2021 list. 

```{r}
unique_words_2020 %>%
  anti_join(stop_words) %>%
  group_by(word) %>%
  tally(sort=TRUE) %>%
  mutate(percent = (n/sum(n))*100) %>%
  top_n(10)

unique_words_2021 %>%
  anti_join(stop_words) %>%
  group_by(word) %>%
  tally(sort=TRUE) %>%
  mutate(percent = (n/sum(n))*100) %>%
  top_n(10)
```

In the 2021 top 10 list, "vaccine" and its variations are much more prominent, which makes sense, while "testing" drops out of the top 10 compared to 2020.

## Going beyond a single word

The next step in text analysis is using `ngrams`. An `ngram` is any combination of words that you specify. Two word ngrams are called bigrams (bi-grams). Three would be trigrams. And so forth.

The code to make ngrams is similar to what we did above, but involves some more twists.

So this block is is going to do the following:

1. Use the covid_tweets data we created above, and filter for pre-2021 tweets.
2. Unnest the tokens again, but instead we're going to create a field called bigram, break apart summary, but we're going to specify the tokens in this case are ngrams of 2.
3. We're going to make things easier to read and split bigrams into word1 and word2.
4. We're going to filter out stopwords again, but this time we're going to do it in both word1 and word2 using a slightly different filtering method.
5. Because of some weirdness in calculating the percentage, we're going to put bigram back together again, now that the stop words are gone.
6. We'll then group by, count and create a percent just like we did above.
7. We'll then use top_n to give us the top 10 bigrams.

### Task 11: Create a dataframe with the top 10 two-word phrases for 2020
**Task** Run the following code and describe the results.
**Answer** This code made another list, but instead of only showing single words it has combined some of the single words from above into phrases. The phrases still have something to do with COVID-19. There is one that seems to have a glitch, but I don't know much about that.

```{r}
covid_tweets %>%
  filter(created < '2021-01-01') %>%
  unnest_tokens(bigram, content, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  mutate(bigram = paste(word1, word2, sep=" ")) %>%
  group_by(bigram) %>%
  tally(sort=TRUE) %>%
  mutate(percent = (n/sum(n))*100) %>%
  top_n(10)
```

And we already have a different, more nuanced result. Health was among the top single words, and we can see that "health care" and "public health" are among the top 2-word phrases. What about after 2021?

### Task 12: Create a dataframe with the top 10 two-word phrases for 2021
**Task** Run the following code and describe the results.
**Answer** Similar to the last list, this list shows phrases instead of single words. This still also has to do with COVID-19, but this list has more phrases that has to do with vaccines than the other one. Also, public health has moved down the list.

```{r}
covid_tweets %>%
  filter(created >= '2021-01-01') %>%
  unnest_tokens(bigram, content, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  mutate(bigram = paste(word1, word2, sep=" ")) %>%
  group_by(bigram) %>%
  tally(sort=TRUE) %>%
  mutate(percent = (n/sum(n))*100) %>%
  top_n(10)
```

While "covid 19" is still the leading phrase, vaccine-related phrases dominate the top 10, and "public health" and "health care" have slipped down the list. You'll notice that the percentages are very small; that's not irrelevant but in some cases it's the differences in patterns that's more important.

So far, we've only looked at the entire set of tweets, not any characteristics of who posted them. Would these lists be any different for Democrats and Republicans? To find out, we just need to add to our filter.

### Task 13: Create a dataframe with the top 10 two-word phrases in 2020 for both Democrats and Republicans
**Task** Run the following code and describe the results.
**Answer** The D dataframe has a lot phrases that have to do with government mandates and actions, like social distancing, relief package and town hall. It also has three listings of variations of pandemic and COVID-19. The R dataframe also has three variations of pandemic and COVID-19, but they aren't as close together. This one had "american people," which I thought was interesting because the other one didn't have that. Also, two listings here have like a glitch : "<U+30FC>." I am assuming this has something to do with how the words are combined.

```{r}
covid_tweets %>%
  filter(created < '2021-01-01', party == 'D') %>%
  unnest_tokens(bigram, content, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  mutate(bigram = paste(word1, word2, sep=" ")) %>%
  group_by(bigram) %>%
  tally(sort=TRUE) %>%
  mutate(percent = (n/sum(n))*100) %>%
  top_n(10)

covid_tweets %>%
  filter(created < '2021-01-01', party == 'R') %>%
  unnest_tokens(bigram, content, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  mutate(bigram = paste(word1, word2, sep=" ")) %>%
  group_by(bigram) %>%
  tally(sort=TRUE) %>%
  mutate(percent = (n/sum(n))*100) %>%
  top_n(10)
```

Now we can begin to see some differences between the parties. We also could do the same for different kinds of accounts: the `title` column represents the role of the account, and if it includes "Candidate" then the tweet is from a campaign account. Let's compare Republican House candidates for 2020 and 2021:

### Task 14: Create a dataframe with the top 10 two-word phrases in 2020 and 2021 for Republican House candidates
**Task** Run the following code and describe the results.
**Answer** The R dataframe has less variations of the word covid than the D dataframe. The top for phrases in the D dataframe have to do with COVID variations. "President Trump" is number two on the R list, while the D list does not have it. The D list has "President Biden," which is pretty low on the list, but the R list does not have that at all. There are also more vaccine mentions on the D list than the R list. There are not mentions of vaccine on the R list. 

```{r}
covid_tweets %>%
  filter(created < '2021-01-01', title == 'House Candidate', party == 'R') %>%
  unnest_tokens(bigram, content, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  mutate(bigram = paste(word1, word2, sep=" ")) %>%
  group_by(bigram) %>%
  tally(sort=TRUE) %>%
  mutate(percent = (n/sum(n))*100) %>%
  top_n(10)

covid_tweets %>%
  filter(created >= '2021-01-01', title == 'House Candidate', party == 'R') %>%
  unnest_tokens(bigram, content, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>%
  mutate(bigram = paste(word1, word2, sep=" ")) %>%
  group_by(bigram) %>%
  tally(sort=TRUE) %>%
  mutate(percent = (n/sum(n))*100) %>%
  top_n(10)
```

There are some differences here, too, but also some potential challenges to doing an analysis. For one, there are variations of words like "vaccine" that could probably be standardized - maybe using OpenRefine - that would give us cleaner results. There might be some words among our list of stop words that actually are meaningful in this context.

## Sentiment Analysis

Another popular use of text analysis is to measure the sentiment of a word - whether it expresses a positive or negative idea - and tidytext has built-in tools to make that possible. We use word counts like we've already calculated and bring in a dataframe of words (called a lexicon) along with their sentiments using a function called `get_sentiments`. The most common dataframe is called "bing" which has nothing to do with the Microsoft search engine. Let's load it:

### Task 15: Load the bing lexicon and produce sentiments for our 2020 and 2021 unique words
**Task** Run the following code and describe the results. Do any of the sentiments seem incorrect or counter-intuitive?
**Answer** In both 2020 and 2021 the sentiment for "Trump" was positive. I don't know if my bias is showing, but I don't recall Trump handling the pandemic very well, so it seems a little incorrect. Additionally, the negative sentiment for "slow" in 2020 seemed a little incorrect to me. I think because this is before vaccinations became available, I thought slow as in the spread of the is slow, which to me would be positive. The other option for slow that would be negative would be the slow vaccine distribution, which was not really a thing at that point. 

```{r}
bing <- get_sentiments("bing")

bing_word_counts_2020 <- unique_words_2020 %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE)

bing_word_counts_2021 <- unique_words_2021 %>%
  inner_join(bing) %>%
  count(word, sentiment, sort = TRUE)

View(bing_word_counts_2020)
View(bing_word_counts_2021)
```

Gauging the sentiment of a word can be heavily dependent on the context, and as with other types of text analysis sometimes larger patterns are more meaningful than individual results. But the potential with text analysis is vast: knowing what words and phrases that public officials employ can be a way to evaluate their priorities, cohesiveness and tactics for persuading voters and their colleagues. And those words and phrases are data.
